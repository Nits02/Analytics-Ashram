{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24c84255-6ea2-42bf-85ac-11a280c73b86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "# --- Date Range Setup (Next 5 Days) ---\n",
    "start_date = datetime.utcnow().date()\n",
    "end_date = start_date + timedelta(days=5)\n",
    "print(f\"üìÖ Getting PredictHQ events from {start_date} to {end_date}\")\n",
    "\n",
    "# --- API Setup ---\n",
    "ACCESS_TOKEN = \"k0LREaYjfPWZ1Hvma3tIKQsNP5BG6RZw35DU7R3p\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {ACCESS_TOKEN}\",\n",
    "    \"Accept\": \"application/json\"\n",
    "}\n",
    "url = \"https://api.predicthq.com/v1/events/\"\n",
    "\n",
    "# --- Params & Pagination ---\n",
    "params = {\n",
    "    \"country\": \"SG\",\n",
    "    \"start.gte\": start_date.isoformat(),\n",
    "    \"start.lte\": end_date.isoformat(),\n",
    "    \"limit\": 100,\n",
    "    \"offset\": 0\n",
    "}\n",
    "all_events = []\n",
    "\n",
    "# --- Loop through paginated responses ---\n",
    "while True:\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"‚ùå Request Failed: {response.status_code} - {response.text}\")\n",
    "\n",
    "    data = response.json()\n",
    "    events = data.get(\"results\", [])\n",
    "    all_events.extend(events)\n",
    "\n",
    "    if not data.get(\"next\"):\n",
    "        break\n",
    "    params[\"offset\"] += params[\"limit\"]\n",
    "\n",
    "# --- Load into pandas DataFrame ---\n",
    "df_pd = pd.DataFrame(all_events)\n",
    "\n",
    "if df_pd.empty:\n",
    "    print(\"‚ö†Ô∏è No events found in the API response.\")\n",
    "else:\n",
    "    # Extract lat/lon from the 'location' field\n",
    "    df_pd[\"lon\"] = df_pd[\"location\"].apply(lambda x: x[0] if isinstance(x, list) else None)\n",
    "    df_pd[\"lat\"] = df_pd[\"location\"].apply(lambda x: x[1] if isinstance(x, list) else None)\n",
    "\n",
    "    # Drop unused columns\n",
    "    df_pd.drop(columns=[\"location\"], inplace=True, errors=\"ignore\")\n",
    "    df_pd.drop(columns=[\"geo\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # --- Convert to Spark DataFrame ---\n",
    "    df_spark = spark.createDataFrame(df_pd)\n",
    "\n",
    "    # Add metadata columns\n",
    "    df_spark = df_spark.withColumn(\"source\", lit(\"PredictHQ\")) \\\n",
    "                       .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "\n",
    "    # --- Preview Data ---\n",
    "    display(df_spark)\n",
    "\n",
    "\n",
    "    # --- Write to Delta Table in Unity Catalog ---\n",
    "    df_spark.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(\"civAI.raw.predict_hq_events\")\n",
    "\n",
    "    print(\"‚úÖ Data written to table: civAI.raw.predict_hq_events\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_predicthq_ingest.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
